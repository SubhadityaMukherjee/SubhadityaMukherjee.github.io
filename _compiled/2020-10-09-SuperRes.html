<!DOCTYPE html>
<html lang="{{ site.lang | default: "en-US" }}">
  <head>
    <meta charset="UTF-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <script type="text/javascript" async
  src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/MathJax.js?config=TeX-MML-AM_CHTML">
</script>
    <link rel="stylesheet" href ="/assets/css/landing.css">
  </head>
  <body>
    <div class="col-md-5">
      <header>
        <h2><a id = "imp" href="../index.html">Home page</a></h2>
       
        <p>Deconstructing Deep Learning +  Î´eviations</p>
        
        <p>
          Drop me an <a href = "mailto: msubhaditya@gmail.com">email</a>
          | RSS feed link : <a href ="https://subhadityamukherjee.github.io/feed.xml">Click</a><br>
          Format : 
          Date | Title<br>
          &emsp;&emsp;TL; DR<br>
          <h4>Total posts : 89</h4>
          <div id="search-container">
            Search for something in the blog <input type="text" id="search-input" placeholder="search...">
          </div><br>
          <ul id="results-container"></ul>
        </p>
        
        
        <p class="view">
        <a href="https://www.github.com/SubhadityaMukherjee">View My GitHub Profile </a>
        </p>
      </header>
      
      <hr>
      <script src="/assets/js/search-script.js" type="text/javascript"></script>
      
      <script>
        SimpleJekyllSearch({
          searchInput: document.getElementById('search-input'),
          resultsContainer: document.getElementById('results-container'),
          json: '/assets/search.json'
        })
        </script>
    </div>
<section>
        <div class=col-md-5>
  <p>Today we will look at Super Resolution in Python.</p>
<p><a href="https://medium.com/@msubhaditya/fixing-small-photos-with-deep-learning-eeae87172a1b?source=friends_link&amp;sk=93f69c860776e76ab641277937bfd886">Medium link</a>
Do you use social media? Ever sent someone a picture and when you look at it later you find that the image is <strong>so</strong> bad in quality? What if you could reverse that?</p>
<h1 id="what-if">What if</h1>
<p>The main objective -&gt; Take an image and upsample it by 3x (That means if an image is initally 100x100 by the end we have a 300x300 image) without losing any information. Now if we decided that we want the image to be 100x100 we could resize it but it would be of a much higher resolution than before.</p>
<h1 id="what-else-could-we-do">What else could we do</h1>
<p>Suppose you do not want to use deep learning for this. What do you do then? Well you have a few options. (None of which I recommend)</p>
<ol>
<li>Become a Photoshop master (although now even that uses Deep learning)</li>
<li>Go back in time and make sure you took a better picture</li>
<li>Deal with it</li>
<li>(Recommended) Read this post</li>
</ol>
<h1 id="where-could-we-use-this">Where could we use this?</h1>
<ul>
<li>Games! Imagine Mario in 1080p :o</li>
<li>Whatsapp junk</li>
<li>Better video calls maybe? Instead of compressing and sending the video, we could take a potentially low resolution video and have it appear in pretty high quality</li>
<li>Have some old pictures you want to upscale? Run it!</li>
</ul>
<h1 id="the-technical-side">The technical side</h1>
<p>Now that we have that out of the way. Let us further define the problem. We will try to explore it from a <a href="https://www.cv-foundation.org/openaccess/content_cvpr_2016/papers/Shi_Real-Time_Single_Image_CVPR_2016_paper.pdf">paper</a>.
Shi, Wenzhe, et al. "Real-time single image and video super-resolution using an efficient sub-pixel convolutional neural network." Proceedings of the IEEE conference on computer vision and pattern recognition. 2016.</p>
<p>The technical version of the objective goes something like this. Consider we have two "spaces". One which is of a high resolution, and another which is of a low resolution. Our network should learn how to convert the pixels from the low resolution space to the other one. Efficiently. </p>
<p>To do this we need to follow standard deep learning training procedures along with a bit of modification. 
(Note that the total code is too long to post here so you can find all of it on my <a href="https://github.com/SubhadityaMukherjee/pytorchTutorialRepo/tree/master/SuperRes">repo</a>)</p>
<h1 id="get-data">Get data.</h1>
<p>Step 1 as usual would be to get data. For our purpose we can use the <a href="http://www2.eecs.berkeley.edu/Research/Projects/CS/vision/bsds/BSDS300-images.tgz">BSDS300 dataset</a>. Just get it and extract it. </p>
<p>Now we need to be able to load the data. To do that, we need a few things. 
- We need to check if the image is a file and load it. We need to perform transforms. And we need to load the data.
- Since most of it is just standard code, let us look at the new things only
- Center Cropping. 
To crop an image we need to identify a valid size to crop to.</p>
<pre><code class="language-py">def calculate_valid_crop_size(crop_size, upscale_factor):
    return crop_size - (crop_size % upscale_factor)
</code></pre>
<p>We can call this using CenterCrop(crop_size) in our transforms pipeline.</p>
<ul>
<li>The main DataLoader. This would enable us to read the dataset and use it for our needs. 
Note that we actually have <strong>two</strong> images. One is the input image, the other is the target we need to convert it to. 
We basically load both of them, perform the required transforms and return them as a <strong>tuple</strong>. This is the important part. And the major difference from something like classification. We return not only one, but two things in the main dataloader.</li>
<li>Now we would have a pair of images, one in the low res space and another in the high res space.</li>
</ul>
<pre><code class="language-py">class DatasetFromFolder(data.Dataset):
    def __init__(self, image_dir, input_transform = None, target_transform =
                 None):
        super(DatasetFromFolder, self).__init__()

        self.image_filenames = [join(image_dir, x) for x in listdir(image_dir)
                               if is_image_file(x)]
        self.input_transform = input_transform
        self.target_transform = target_transform

    def __getitem__(self, index):
        input = load_img(self.image_filenames<ul>
<li>
        <a href = ./_compiled/2021-03-09-AISuperpowersKaiFuLee.html>AI Superpowers Kai Fu Lee:2021-03-09T09:04+04:00
summary</li><li>
        <a href = ./_compiled/2021-03-07-DigitalMinimalismCalNewport.html>Digital Minimalism Cal Newport:2021-03-07T16:24+04:00
summary</li><li>
        <a href = ./_compiled/2021-03-05-tricksFromLectures.html>More Deep Learning, Less Crying - A guide:2021-03-05T18:42+04:00
summary</li><li>
        <a href = ./_compiled/2021-03-04-FP16.html>FP16:2021-04-01T00:22+04:00
summary</li><li>
        <a href = ./_compiled/2021-02-25-swish.html>swish:2020-06-19 16:53:45 +0400
summary</li><li>
        <a href = ./_compiled/2021-02-25-shufflenet.html>ShuffleNet:2020-06-19 16:53:45 +0400
summary</li><li>
        <a href = ./_compiled/2021-02-25-selu.html>SELU:2020-06-19 16:53:45 +0400
summary</li><li>
        <a href = ./_compiled/2021-02-25-randomErasingAugmentation.html>Random erasing data augmentation:2020-06-19 16:53:45 +0400
summary</li><li>
        <a href = ./_compiled/2021-02-25-bagOfTricks.html>Bag Of Tricks:2020-06-19 16:53:45 +0400
summary</li><li>
        <a href = ./_compiled/2021-02-25-WGAN.html>WGAN:2020-06-19 16:53:45 +0400
summary</li><li>
        <a href = ./_compiled/2021-02-25-VGGNet.html>VGG net:2020-06-19 16:53:45 +0400
summary</li><li>
        <a href = ./_compiled/2021-02-25-VAE.html>VAE (Auto-Encoding Variational Bayes):2020-06-19 16:53:45 +0400
summary</li><li>
        <a href = ./_compiled/2021-02-25-Unet.html>Unets:2020-06-19 16:53:45 +0400
summary</li><li>
        <a href = ./_compiled/2021-02-25-Thinking machines - Turing.html>Thinking machines - Turing (Just notes):2020-06-19 16:53:45 +0400
summary</li><li>
        <a href = ./_compiled/2021-02-25-SuperResolution.html>Super Resolution using Sub Pixel Convolutions:2020-06-19 16:53:45 +0400
summary</li><li>
        <a href = ./_compiled/2021-02-25-SqueezeNet.html>Squeeze Net:2020-06-19 16:53:45 +0400
summary</li><li>
        <a href = ./_compiled/2021-02-25-Spatial Transformer Network.html>Spatial Transformer Networks:2020-06-19 16:53:45 +0400
summary</li><li>
        <a href = ./_compiled/2021-02-25-SRCNN.html>SRCNN:2020-06-19 16:53:45 +0400
summary</li><li>
        <a href = ./_compiled/2021-02-25-Rethinking Generalization.html>Understanding Deep learning requires rethinking generalization (Just notes):2020-06-19 16:53:45 +0400
summary</li><li>
        <a href = ./_compiled/2021-02-25-ResNet.html>GoogLe Net:2020-06-19 16:53:45 +0400
summary</li><li>
        <a href = ./_compiled/2021-02-25-Pruning.html>What is the state of pruning:2020-06-19 16:53:45 +0400
summary</li><li>
        <a href = ./_compiled/2021-02-25-Perceptual Loss.html>Perceptual Loss:2020-06-19 16:53:45 +0400
summary</li><li>
        <a href = ./_compiled/2021-02-25-OneCycle.html>One cycle paper:2020-06-19 16:53:45 +0400
summary</li><li>
        <a href = ./_compiled/2021-02-25-NVAE.html>NVAE (WIP):2020-06-19 16:53:45 +0400
summary</li><li>
        <a href = ./_compiled/2021-02-25-NN hyper-parameters disciplined approach.html>A disciplined approach to neural network hyper-parameters:2020-06-19 16:53:45 +0400
summary</li><li>
        <a href = ./_compiled/2021-02-25-MobileNet.html>Mobile Net:2020-06-19 16:53:45 +0400
summary</li><li>
        <a href = ./_compiled/2021-02-25-LightSeg.html>LightSeg (only notes for now):2020-06-19 16:53:45 +0400
summary</li><li>
        <a href = ./_compiled/2021-02-25-LSTM.html>LSTM:2020-06-19 16:53:45 +0400
summary</li><li>
        <a href = ./_compiled/2021-02-25-HRNet.html>HRNet (WIP):2020-06-19 16:53:45 +0400
summary</li><li>
        <a href = ./_compiled/2021-02-25-GoogLeNet.html>GoogLe Net:2020-06-19 16:53:45 +0400
summary</li><li>
        <a href = ./_compiled/2021-02-25-Focal Loss.html>Focal Loss:2020-06-19 16:53:45 +0400
summary</li><li>
        <a href = ./_compiled/2021-02-25-FederatedLearningGboard.html>Google Keyboard Federated Learning(Notes only. refer to [35] for code):2020-06-19 16:53:45 +0400
summary</li><li>
        <a href = ./_compiled/2021-02-25-FederatedLearning.html>Federated Learning (original paper):2020-06-19 16:53:45 +0400
summary</li><li>
        <a href = ./_compiled/2021-02-25-Dropout.html>Dropout (Just notes):2020-06-19 16:53:45 +0400
summary</li><li>
        <a href = ./_compiled/2021-02-25-DeepLab (Semantic seg).html>Semantic segmentation DeepLab:2020-06-19 16:53:45 +0400
summary</li><li>
        <a href = ./_compiled/2021-02-25-DeepDream.html>Inceptionism (Google Deep Dream):2020-06-19 16:53:45 +0400
summary</li><li>
        <a href = ./_compiled/2021-02-25-DRlNeuro.html>DRL and neuroscience (Just notes:2020-06-19 16:53:45 +0400
summary</li><li>
        <a href = ./_compiled/2021-02-25-DCGan.html>DC GAN:2020-06-19 16:53:45 +0400
summary</li><li>
        <a href = ./_compiled/2021-02-25-ComputationalLimits.html>Computational Limits (Just notes):2020-06-19 16:53:45 +0400
summary</li><li>
        <a href = ./_compiled/2021-02-25-Class Imbalance.html>Class Imbalance Problem:2020-06-19 16:53:45 +0400
summary</li><li>
        <a href = ./_compiled/2021-02-25-AlexNet.html>Alex Net:2020-06-19 16:53:45 +0400
summary</li><li>
        <a href = ./_compiled/2020-10-09-SuperRes.html>Super resolution:2020-10-09T22:01+04:00
summary</li><li>
        <a href = ./_compiled/2020-10-07-FederatedLearning.html>Federated Learning:2020-10-07T20:15+04:00
summary</li><li>
        <a href = ./_compiled/2020-10-03-TakingBatchnormForGranted.html>Taking Batchnorm For Granted:2020-10-03T19:52+04:00
summary</li><li>
        <a href = ./_compiled/2020-09-28-AdversarialAttack.html>A murder mystery and Adversarial attack:2020-09-28T19:57+04:00
summary</li><li>
        <a href = ./_compiled/2020-09-26-An.html>Thank you and a rain check:2020-09-26T19:16+04:00
summary</li><li>
        <a href = ./_compiled/2020-09-25-Pruning.html>Pruning:2020-09-25T23:02+04:00
summary</li><li>
        <a href = ./_compiled/2020-09-05-Documentation.html>Documentation using Documenter.jl:2020-09-05T00:41+04:00
summary</li><li>
        <a href = ./_compiled/2020-09-02-DatasetBindings.html>Dataset Bindings:2020-09-02T09:29+04:00
summary</li><li>
        <a href = ./_compiled/2020-08-26-DCGAN.html>DCGAN:2020-08-26T00:03+04:00
summary</li><li>
        <a href = ./_compiled/2020-08-13-Differentiable Programming.html>Differentiable Programming:2020-08-13 01:07:58 +0400
summary</li><li>
        <a href = ./_compiled/2020-08-12-Compositional Pattern Producing Networks.html>Compositional Pattern Producing Networks:2020-08-12 00:26:21 +0400
summary</li><li>
        <a href = ./_compiled/2020-08-11-VGG.html>VGG:2020-08-11 23:36:56 +0400
summary</li><li>
        <a href = ./_compiled/2020-07-29-Optimizers.html>Optimizers:2020-07-29 01:21:51 +0400
summary</li><li>
        <a href = ./_compiled/2020-07-25-convFlux.html>Simple conv with Flux:2020-07-25 20:35:29 +0400
summary</li><li>
        <a href = ./_compiled/2020-07-25-VAE.html>VAE:2020-07-25 22:50:19 +0400
summary</li><li>
        <a href = ./_compiled/2020-07-24-rnnFromScratch.html>RNN from scratch:2020-07-24 11:41:02 +0400
summary</li><li>
        <a href = ./_compiled/2020-07-24-backprop.html>Backprop:2020-07-24 09:51:59 +0400
summary</li><li>
        <a href = ./_compiled/2020-07-21-Low Poly.html>Low Poly:2020-07-21 09:42:30 +0400
summary</li><li>
        <a href = ./_compiled/2020-07-18-Universal Approximation theorem.html>Universal Approximation theorem:2020-07-18 10:42:41 +0400
summary</li><li>
        <a href = ./_compiled/2020-07-17-preprocessing.html>Image Preprocessing:2020-07-17 00:56:06 +0400
summary</li><li>
        <a href = ./_compiled/2020-07-12-fastai Book.html>fastai Book:2020-07-12 11:50:52 +0400
summary</li><li>
        <a href = ./_compiled/2020-07-11-pooling.html>Pooling layers:2020-07-11 12:43:34 +0400
summary</li><li>
        <a href = ./_compiled/2020-07-10-evenFasterConv.html>My life is a lie + Even faster conv:2020-07-10 22:24:36 +0400
summary</li><li>
        <a href = ./_compiled/2020-07-09-mandelbrot.html>Mandelbrot set:2020-07-09 16:51:12 +0400
summary</li><li>
        <a href = ./_compiled/2020-07-07-adopt .html>Adopt dont shop:2020-07-07 02:04:45 +0400
summary</li><li>
        <a href = ./_compiled/2020-07-07-Action Recognition part 2.html>Action Recognition part 2:2020-07-07 13:57:20 +0400
summary</li><li>
        <a href = ./_compiled/2020-07-06-Video Classification.html>Action recognition part 1:2020-07-06 20:34:52 +0400
summary</li><li>
        <a href = ./_compiled/2020-07-06-Endangered Species.html>Endangered Species:2020-07-06 01:59
summary</li><li>
        <a href = ./_compiled/2020-07-04-padding.html>Padding:2020-07-04 11:25:39 +0400
summary</li><li>
        <a href = ./_compiled/2020-07-03-fasterConv.html>Faster and more general Conv:2020-07-03 17:13:46 +0400
summary</li><li>
        <a href = ./_compiled/2020-07-02-kernels.html>Image kernels:2020-07-02 19:34:06 +0400
summary</li><li>
        <a href = ./_compiled/2020-07-02-convolution.html>Basic Convolution:2020-07-02 17:38:00 +0400
summary</li><li>
        <a href = ./_compiled/2020-07-01-time.html>Imposter Syndrome in the research community:2020-07-01 15:03:45 +0400
summary</li><li>
        <a href = ./_compiled/2020-06-30-loss functions.html>Loss Functions:2020-06-30 00:01:59 +0400
summary</li><li>
        <a href = ./_compiled/2020-06-30-latex from code.html>latex from code:2020-06-30 17:24:21 +0400
summary</li><li>
        <a href = ./_compiled/2020-06-27-Implementing Papers.html>Implementing Papers:2020-06-27 11:56:57 +0400
summary</li><li>
        <a href = ./_compiled/2020-06-27-100PageMlbook.html>100PageMlblook:2020-06-27 11:44:28 +0400
summary</li><li>
        <a href = ./_compiled/2020-06-24-SGD.html>SGD:2020-06-24 11:30:33 +0400
summary</li><li>
        <a href = ./_compiled/2020-06-23-batching.html>Batching:2020-06-23 18:48:06 +0400
summary</li><li>
        <a href = ./_compiled/2020-06-21-initialization.html>Initialization:2020-06-21 13:58:32 +0400
summary</li><li>
        <a href = ./_compiled/2020-06-20-layers.html>Linear Model:2020-06-20 12:43:30 +0400
summary</li><li>
        <a href = ./_compiled/2020-06-20-activationFunctions.html>Activation functions:2020-06-20 02:00:43 +0400
summary</li><li>
        <a href = ./_compiled/2020-06-19-trainTest.html>Oversample and Split:2020-06-19 17:52:02 +0400
summary</li><li>
        <a href = ./_compiled/2020-06-19-notebook2script.html>notebook2script:2020-06-19 19:10:30 +0400
summary</li><li>
        <a href = ./_compiled/2020-06-19-dataloader.html>Dataloader:2020-06-19 16:53:45 +0400
summary</li><li>
        <a href = ./_compiled/2020-06-19-Vision.html>Vision:2020-06-19 21:56:03 +0400
summary</li><li>
        <a href = ./_compiled/2020-06-19-Initial-Steps.html>"Initial Steps":
summary</li><li>
        <a href = ./_compiled/2020-06-19-Defining-the-problem.html>"Defining the project and outlining what is to come in the future":
summary</li>
<ul>
)
        target = input.copy()
        if self.input_transform:
            input = self.input_transform(input)
        if self.target_transform:
            target = self.target_transform(target)

        return input, target

    def __len__(self):
        return len(self.image_filenames)
</code></pre>
<h1 id="create-the-network">Create the network</h1>
<p>Welcome to the deep end. 
Our network is actually simple since we are using pytorch. We have 4 conv layers, 4 ReLUs and a special layer called PixelShuffle.</p>
<p>What is PixelShuffle. Well it is the defining moment for the paper we are considering. So defining in fact that PyTorch actually has it inbuilt. In simple terms, this layer is a shuffler. It takes the the tensor of shape H(height) x W(width) x C(channel) . r^2(no of activation) and gives us back a tensor of shape rH x rW x rC. "Shuffle" aka a sub pixel convolutional layer. This is useful because now everything is parallelizable. </p>
<p>We also need to initalize our weights. This is done to make sure that our network starts off on a good foot. We use orthogonal initialization here.</p>
<pre><code class="language-py">import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.nn.init as init

# Main network
class Net(nn.Module):
    def __init__(self, upscale_factor):
        super(Net, self).__init__()
        self.relu = nn.ReLU()
        self.conv1 = nn.Conv2d(1, 64, (5,5), (1,1), (2,2))
        self.conv2 = nn.Conv2d(64, 64, (3,3), (1,1), (1,1))
        self.conv3 = nn.Conv2d(64, 32, (3,3), (1,1), (1,1))
        self.conv4 = nn.Conv2d(32, upscale_factor**2, (3,3), (1,1), (1,1))
        self.pixel_shuffle = nn.PixelShuffle(upscale_factor)

        self._initialize_weights()

    def forward(self, x):
        x = self.relu(self.conv1(x))
        x = self.relu(self.conv2(x))
        x = self.relu(self.conv3(x))
        x = self.pixel_shuffle(self.conv4(x))
        return x

    def _initialize_weights(self):
        init.orthogonal_(self.conv1.weight, init.calculate_gain('relu'))
        init.orthogonal_(self.conv2.weight,init.calculate_gain('relu'))
        init.orthogonal_(self.conv3.weight,init.calculate_gain('relu'))
        init.orthogonal_(self.conv4.weight)
</code></pre>
<h1 id="train">Train!</h1>
<p>Wow, we are almost halfway done. Now for the main training. We first load everything we need, import our Net with an upscale factor (aka how many x we need to upscale).
We then initalize our Optimizer. We will be using Adam here because it works the best.
We also need a loss function. </p>
<p>Well if you think about it, since we are trying to find a pixel wise difference, why not use exactly that.. MSELoss.</p>
<p>We follow the simple strategy of : batch -&gt; push to GPU -&gt; zero the optimizers gradients -&gt; Calculate the loss -&gt; Backprop -&gt; Step through the optimizer.
(P.S If there is anything here you do not understand, have a look at my <a href="https://www.subhadityamukherjee.me/deconstructingdl.html">blog</a>)</p>
<pre><code class="language-py">def train(args, device, train_loader,model,  epoch, optimizer, criterion):
    epoch_loss = 0
    device = torch.device(&quot;cuda&quot;) # Sending to GPU
    for batch_idx, batch in tqdm(enumerate(train_loader, 1)):
        input, target = batch[0].to(device), batch[1].to(device)
        optimizer.zero_grad() # zero gradients
        loss = criterion(model(input), target) # calc loss
        epoch_loss += loss.item()
        loss.backward() #backprop
        optimizer.step()

        print(f&quot;Iteration: {batch_idx}, Loss: {loss} &quot;)
    print(f&quot;Avg epoch_loss: {epoch/len(train_loader)}&quot;)
</code></pre>
<p>The test loop is very similar to that here so no need to talk about it specifically.
We save the model after it is done training. </p>
<h1 id="run-it">Run it!</h1>
<p>Now that we have a really cool model which works. How about putting it to use? </p>
<p>To do that, let us take an image, preprocess it, convert it to a tensor. After that we can load the model and send this image to the GPU. </p>
<pre><code class="language-py">img = Image.open(opt.input_image).convert('YCbCr')
y, cb, cr = img.split()

model = torch.load(opt.model)
img_to_tensor = ToTensor()
input = img_to_tensor(y).view(1, -1, y.size[1], y.size[0])

if opt.cuda:
        model = model.cuda()
        input = input.cuda()
</code></pre>
<p>Then we pass it through our model. We get a weird output now. To fix that, we make it a numpy array and perform some operations which will make our image into a proper one. Basically we alter the values so that we have a range in the RGB value range. This will allow us to plot it or do whatever we want.</p>
<pre><code class="language-py">out = model(input)
out = out.cpu()
out_img_y = out[0].detach().numpy()
out_img_y *= 255.0
out_img_y = out_img_y.clip(0, 255)
out_img_y = Image.fromarray(np.uint8(out_img_y[0]), mode='L')
</code></pre>
<p>We can now resize and save the image to wherever we want to.</p>
<pre><code class="language-py">out_img_cb = cb.resize(out_img_y.size, Image.BICUBIC)
out_img_cr = cr.resize(out_img_y.size, Image.BICUBIC)
out_img = Image.merge('YCbCr', [out_img_y, out_img_cb,
                            out_img_cr]).convert('RGB')

out_img.save(opt.output_filename)
print('output image saved to ', opt.output_filename)
</code></pre>
<p>And we are done!</p>
<h1 id="bonus">Bonus</h1>
<p>I also converted the script to work with videos. :)
Go check out the <a href="https://github.com/SubhadityaMukherjee/pytorchTutorialRepo/tree/master/SuperRes">repo</a></p>
<h1 id="some-thoughts-from-the-paper">Some thoughts from the paper</h1>
<ul>
<li>Super res is used in Medical imagery, face recognition, satellite images</li>
<li>We aim to learn implicit redundancy in the data</li>
<li>The filters learnt are a much better representation of the upsampling than a single filter</li>
<li>Using variable learning rates from 0.01 to 0.0001 works very well</li>
<li>Each pixel in the original image is represented only once to preserve the efficiency of the network</li>
</ul>
<h1 id="next-steps">Next steps</h1>
<p>Thank you for reading this far! Hope you liked the article and it helped you understand the topic better. I would sincerely recommend you read the paper. It is an easy read. 
Super resolution is something I have been interested in a long time and just today I found an <a href="https://github.com/jixiaozhong/RealSR">even better network</a> for this task.</p>
<p>Do reach out if you liked the article or have any feedback for future ones. Hope you have a great day! </p>
        </div>
  </body>
</html>


<!-- --- -->
<!-- layout: default -->
<!-- --- -->
<!-- <a href = "/deeplearning.html">Go to index</a><br><br> -->
<!--  -->
<!--  -->
<!-- <h1>{{ page.title }}</h1> -->
<!--  -->
<!-- <span class="reading-time" title="Estimated read time"> -->
<!--   {% assign words = content | number_of_words %} -->
<!--   {% if words < 360 %} -->
<!--     <h3>Reading time : ~1 min</h3> -->
<!--   {% else %} -->
<!--     <h3>Reading time : ~{{ words | divided_by:100 }} mins</h3> -->
<!--   {% endif %} -->
<!-- </span> -->
<!--  -->
<!--  -->
<!-- <p class="view">by {{ page.author | default: site.author }}</p> -->
<!-- {% include toc.html html=content %} -->
<!-- {{ content }} -->
<!--  -->
<!-- <section> -->
<!-- Related posts:&emsp; -->
<!-- {% for p in site.related_posts %} -->
<!--   <a href={{ p.url }}> {{ p.title }}&emsp; </a> -->
<!-- {% endfor %} -->
<!--  -->
<!-- </section> -->
<!--  -->
