---
layout: post
title:  SGD
date:   2020-06-24 11:30:33 +0400
tags: stochastic gradient descent optimize
---

In this post we will try to implement SGD and read a bit about what it is.

Before that. Let us review what we have so far.
1. A way to read images and identify classes
2. A generator go give us this data in batches
3. A dense layer
4. A bunch of activation functions
5. A basic backprop loop.

So what is SGD? Well it is a tiny addition to the loop in which we calculate the gradient and update the weight matrix batch wise instead of doing it for the whole data at once. Thats about it.

Why? Because we have a limited memory. And we are "streaming" the data so to speak. SGD is one of the older technqiues. It works. It works pretty well and it is the first step towards making a proper DL pipeline.

So in the case of gradient descent, SGD just takes random values instead aka Stochastic.

How do we implement it? Let us modify our training loop a bit now and add what we had discussed for "Linear Model". Lets see if it works. Also as a note. I am using xtest/ytest because it is smaller and we are not using a GPU right now.


