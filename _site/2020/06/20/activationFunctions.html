<!DOCTYPE html>
<html lang="en-US">
  <head>
    <meta charset="UTF-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1">

<!-- Begin Jekyll SEO tag v2.6.1 -->
<title>Activation functions | Deconstructing Deep learning</title>
<meta name="generator" content="Jekyll v3.8.7" />
<meta property="og:title" content="Activation functions" />
<meta name="author" content="Subhaditya Mukherjee" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="Activation functions are an extremely important part of any neural network. But they are actually much simpler than we make them out to be. Here are some of them. Lets define a test matrix." />
<meta property="og:description" content="Activation functions are an extremely important part of any neural network. But they are actually much simpler than we make them out to be. Here are some of them. Lets define a test matrix." />
<link rel="canonical" href="http://localhost:4000/2020/06/20/activationFunctions.html" />
<meta property="og:url" content="http://localhost:4000/2020/06/20/activationFunctions.html" />
<meta property="og:site_name" content="Deconstructing Deep learning" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2020-06-20T02:00:43+04:00" />
<script type="application/ld+json">
{"@type":"BlogPosting","url":"http://localhost:4000/2020/06/20/activationFunctions.html","author":{"@type":"Person","name":"Subhaditya Mukherjee"},"headline":"Activation functions","dateModified":"2020-06-20T02:00:43+04:00","datePublished":"2020-06-20T02:00:43+04:00","description":"Activation functions are an extremely important part of any neural network. But they are actually much simpler than we make them out to be. Here are some of them. Lets define a test matrix.","mainEntityOfPage":{"@type":"WebPage","@id":"http://localhost:4000/2020/06/20/activationFunctions.html"},"@context":"https://schema.org"}</script>
<!-- End Jekyll SEO tag -->

    <link rel="stylesheet" href="/assets/css/style.css?v=af7cc59f07c9fcc20fef213b005cd53e62e631d8">
    <!--[if lt IE 9]>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7.3/html5shiv.min.js"></script>
    <![endif]-->
  </head>
  <body>
    <div class="wrapper">
      <header>
        <h1><a href="https://subhadityamukherjee.github.io/deconstructingdl.html">Home</a></h1>
	<h1><a href="https://subhadityamukherjee.github.io/">About me</a></h1>
	<h1><a href = "mailto: msubhaditya@gmail.com">Drop me an email</a></h1>
        

        <p>Making a Deep Learning library from scratch in Julia and documenting it the whole way!</p>

        
        <p class="view"><a href="https://github.com/SubhadityaMukherjee/DataLoader.jl">View the Project on GitHub <small>github.com/SubhadityaMukherjee/DataLoader.jl</small></a></p>
        

        

        
      </header>
      <section>
      <!-- Html Elements for Search -->
<div id="search-container">
Search for something in the blog <input type="text" id="search-input" placeholder="search...">
<ul id="results-container"></ul>
</div>

<!-- Script pointing to search-script.js -->
<script src="js/search-script.js" type="text/javascript"></script>

<!-- Configuration -->
<script>
SimpleJekyllSearch({
  searchInput: document.getElementById('search-input'),
  resultsContainer: document.getElementById('results-container'),
  json: '/search.json'
})
</script>

      <small>20 June 2020</small>
<h1>Activation functions</h1>

<p class="view">by Subhaditya Mukherjee</p>

<p>Activation functions are an extremely important part of any neural network. But they are actually much simpler than we make them out to be. Here are some of them.
Lets define a test matrix.</p>

<div class="language-julia highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">test</span> <span class="o">=</span> <span class="x">[</span><span class="mi">100</span> <span class="mf">1.0</span> <span class="mf">0.0</span> <span class="o">-</span><span class="mf">300.0</span><span class="x">;</span><span class="mi">100</span> <span class="mf">1.0</span> <span class="mf">0.0</span> <span class="mf">300.0</span><span class="x">]</span>
</code></pre></div></div>

<h2 id="relu">Relu</h2>
<ul>
  <li>The Rectified Linear Unit (ReLU) activation function produces 0 as an output when x &lt; 0, and then produces a linear with slope of 1 when x &gt; 0.</li>
  <li>Nair, V., &amp; Hinton, G. E. (2010, January). Rectified linear units improve restricted boltzmann machines. In ICML.</li>
  <li>f(x) = max(0,x)</li>
</ul>

<div class="language-julia highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">relu</span><span class="x">(</span><span class="n">mat</span><span class="x">)</span> <span class="o">=</span> <span class="n">max</span><span class="o">.</span><span class="x">(</span><span class="mi">0</span><span class="x">,</span> <span class="n">mat</span><span class="x">)</span>
</code></pre></div></div>

<h2 id="leaky-relu">Leaky relu</h2>
<ul>
  <li>Andrew L. Maas, Awni Y. Hannun, Andrew Y. Ng (2014). Rectifier Nonlinearities Improve Neural Network Acoustic Models.</li>
  <li>f(x) = max(0.01x,x)</li>
</ul>

<div class="language-julia highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">lrelu</span><span class="x">(</span><span class="n">x</span><span class="x">)</span> <span class="o">=</span> <span class="n">max</span><span class="o">.</span><span class="x">(</span><span class="mf">0.01</span><span class="n">x</span><span class="x">,</span> <span class="n">x</span><span class="x">)</span>
</code></pre></div></div>

<h2 id="prelu">PRelu</h2>
<ul>
  <li>f(x) = max(x, x*a)</li>
</ul>

<div class="language-julia highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c">#export</span>
<span class="n">prelu</span><span class="x">(</span><span class="n">x</span><span class="x">,</span><span class="n">a</span><span class="x">)</span> <span class="o">=</span> <span class="n">max</span><span class="o">.</span><span class="x">(</span><span class="n">x</span><span class="x">,</span> <span class="n">x</span><span class="o">.*</span><span class="n">a</span><span class="x">)</span>
<span class="n">prelu</span><span class="x">(</span><span class="n">test</span><span class="x">,</span><span class="mf">0.10</span><span class="x">)</span>
</code></pre></div></div>

<h2 id="maxout">Maxout</h2>
<ul>
  <li>f(x) = max(x, x*a)</li>
</ul>

<div class="language-julia highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">maxout</span><span class="x">(</span><span class="n">x</span><span class="x">,</span><span class="n">a</span><span class="x">)</span> <span class="o">=</span> <span class="n">max</span><span class="o">.</span><span class="x">(</span><span class="n">x</span><span class="x">,</span> <span class="n">x</span><span class="o">.*</span><span class="n">a</span><span class="x">)</span>
<span class="n">maxout</span><span class="x">(</span><span class="n">test</span><span class="x">,</span><span class="mf">0.10</span><span class="x">)</span>
</code></pre></div></div>

<h2 id="sigmoid">Sigmoid</h2>
<ul>
  <li>f(x) = 1/(1+e^-x)</li>
</ul>

<div class="language-julia highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">σ</span><span class="x">(</span><span class="n">x</span><span class="x">)</span> <span class="o">=</span> <span class="mi">1</span> <span class="o">./</span><span class="x">(</span><span class="mi">1</span> <span class="o">.+</span><span class="n">exp</span><span class="o">.</span><span class="x">(</span><span class="o">-</span><span class="n">x</span><span class="x">))</span>
<span class="n">σ</span><span class="x">(</span><span class="n">test</span><span class="x">)</span>
</code></pre></div></div>

<h2 id="noisy-relu">Noisy Relu</h2>
<ul>
  <li>f(x) = max(0, x+Y) where YϵNormal(0,1)</li>
</ul>

<div class="language-julia highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">using</span> <span class="n">Distributions</span>
<span class="n">noisyrelu</span><span class="x">(</span><span class="n">x</span><span class="x">)</span> <span class="o">=</span> <span class="n">max</span><span class="o">.</span><span class="x">(</span><span class="mi">0</span><span class="x">,</span> <span class="n">x</span><span class="o">.+</span><span class="n">rand</span><span class="x">(</span><span class="n">Distributions</span><span class="o">.</span><span class="n">Normal</span><span class="x">(),</span> <span class="mi">1</span><span class="x">))</span>
<span class="n">noisyrelu</span><span class="x">(</span><span class="n">test</span><span class="x">)</span>
</code></pre></div></div>

<h2 id="softplus">Softplus</h2>
<ul>
  <li>f(x) = log(e^x+1)</li>
</ul>

<div class="language-julia highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">softplus</span><span class="x">(</span><span class="n">x</span><span class="x">)</span> <span class="o">=</span> <span class="n">log</span><span class="o">.</span><span class="x">(</span><span class="n">exp</span><span class="o">.</span><span class="x">(</span><span class="n">test</span><span class="x">)</span><span class="o">.+</span><span class="mi">1</span><span class="x">)</span>
<span class="n">softplus</span><span class="x">(</span><span class="n">test</span><span class="x">)</span>
</code></pre></div></div>

<h2 id="elu">Elu</h2>
<ul>
  <li>f(x) = max(x, a*(e^x-1))</li>
</ul>

<div class="language-julia highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">elu</span><span class="x">(</span><span class="n">x</span><span class="x">,</span><span class="n">a</span><span class="x">)</span> <span class="o">=</span> <span class="n">max</span><span class="o">.</span><span class="x">(</span><span class="n">x</span><span class="x">,</span> <span class="n">a</span><span class="o">.*</span><span class="x">(</span><span class="n">exp</span><span class="o">.</span><span class="x">(</span><span class="n">x</span><span class="x">)</span> <span class="o">.-</span><span class="mi">1</span><span class="x">))</span>
<span class="n">elu</span><span class="x">(</span><span class="n">test</span><span class="x">,</span><span class="mf">0.1</span><span class="x">)</span>
</code></pre></div></div>

<h2 id="swish">Swish</h2>
<ul>
  <li>f(x) = x/(1+e^(-βx))</li>
</ul>

<div class="language-julia highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">swish</span><span class="x">(</span><span class="n">x</span><span class="x">,</span><span class="n">β</span><span class="x">)</span> <span class="o">=</span> <span class="n">x</span> <span class="o">./</span><span class="x">(</span><span class="mi">1</span> <span class="o">.+</span><span class="n">exp</span><span class="o">.</span><span class="x">(</span><span class="o">-</span><span class="n">β</span><span class="o">.*</span><span class="n">x</span><span class="x">))</span>
<span class="n">swish</span><span class="x">(</span><span class="n">test</span><span class="x">,</span><span class="mf">0.1</span><span class="x">)</span>
</code></pre></div></div>



  <small>tags: <em>activation</em> - <em>leaky</em> - <em>swish</em> - <em>sigmoid</em> - <em>soft</em> - <em>elu</em></small>



      </section>
      <footer>
        
        <p>This project is maintained by <a href="https://github.com/SubhadityaMukherjee">SubhadityaMukherjee</a></p>
        
      </footer>
    </div>
    <script src="/assets/js/scale.fix.js"></script>
    
  </body>
</html>
