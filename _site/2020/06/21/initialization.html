<!DOCTYPE html>
<html lang="en-US">
  <head>
    <meta charset="UTF-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1">

<!-- Begin Jekyll SEO tag v2.6.1 -->
<title>Initialization | Deconstructing Deep learning</title>
<meta name="generator" content="Jekyll v3.8.7" />
<meta property="og:title" content="Initialization" />
<meta name="author" content="Subhaditya Mukherjee" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="We explore the different initialization techniques that we have and look at papers to see which does better." />
<meta property="og:description" content="We explore the different initialization techniques that we have and look at papers to see which does better." />
<link rel="canonical" href="http://localhost:4000/2020/06/21/initialization.html" />
<meta property="og:url" content="http://localhost:4000/2020/06/21/initialization.html" />
<meta property="og:site_name" content="Deconstructing Deep learning" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2020-06-21T13:58:32+04:00" />
<script type="application/ld+json">
{"@type":"BlogPosting","url":"http://localhost:4000/2020/06/21/initialization.html","author":{"@type":"Person","name":"Subhaditya Mukherjee"},"headline":"Initialization","dateModified":"2020-06-21T13:58:32+04:00","datePublished":"2020-06-21T13:58:32+04:00","description":"We explore the different initialization techniques that we have and look at papers to see which does better.","mainEntityOfPage":{"@type":"WebPage","@id":"http://localhost:4000/2020/06/21/initialization.html"},"@context":"https://schema.org"}</script>
<!-- End Jekyll SEO tag -->

    <link rel="stylesheet" href="/assets/css/style.css?v=18c61fa44ac8eed92ed8e907e200a346966e268d">
    <!--[if lt IE 9]>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7.3/html5shiv.min.js"></script>
    <![endif]-->
  </head>
  <body>
    <div class="wrapper">
      <header>
        <h1><a href="https://subhadityamukherjee.github.io/deconstructingdl.html">Home</a></h1>
	<h1><a href="https://subhadityamukherjee.github.io/">About me</a></h1>
	<h1><a href = "mailto: msubhaditya@gmail.com">Drop me an email</a></h1>
        

        <p>Making a Deep Learning library from scratch in Julia and documenting it the whole way!</p>

        
        <p class="view"><a href="https://github.com/SubhadityaMukherjee/DataLoader.jl">View the Project on GitHub <small>github.com/SubhadityaMukherjee/DataLoader.jl</small></a></p>
        

        

        
      </header>
      <section>
      <!-- Html Elements for Search -->
<div id="search-container">
Search for something in the blog <input type="text" id="search-input" placeholder="search...">
<ul id="results-container"></ul>
</div>

<!-- Script pointing to search-script.js -->
<script src="js/search-script.js" type="text/javascript"></script>

<!-- Configuration -->
<script>
SimpleJekyllSearch({
  searchInput: document.getElementById('search-input'),
  resultsContainer: document.getElementById('results-container'),
  json: '/search.json'
})
</script>

      <small>21 June 2020</small>
<h1>Initialization</h1>

<p class="view">by Subhaditya Mukherjee</p>

<p>We explore the different initialization techniques that we have and look at papers to see which does better.</p>

<p>Here goes!</p>

<ul>
  <li>Zero Initialization: set all weights to 0
Please dont. I mean its the worst idea. But anyway.</li>
</ul>

<div class="language-julia highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">W</span> <span class="o">=</span> <span class="n">zeros</span><span class="x">(</span><span class="mi">2</span><span class="x">,</span><span class="mi">100</span><span class="x">)</span>
<span class="n">b</span> <span class="o">=</span> <span class="n">zeros</span><span class="x">(</span><span class="mi">2</span><span class="x">)</span>
</code></pre></div></div>

<ul>
  <li>Normal Initialization: set all weights to random small numbers
This is what we did as a test. It does better than init 0 but still. Not a great idea.</li>
</ul>

<div class="language-julia highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">W</span> <span class="o">=</span> <span class="n">rand</span><span class="x">(</span><span class="mi">2</span><span class="x">,</span><span class="mi">100</span><span class="x">)</span>
<span class="n">b</span> <span class="o">=</span> <span class="n">rand</span><span class="x">(</span><span class="mi">2</span><span class="x">)</span>
</code></pre></div></div>

<ul>
  <li>Lecun Initialization: normalize variance
LeCun, Y. A., Bottou, L., Orr, G. B., &amp; Müller, K. R. (2012). Efficient backprop. In Neural networks: Tricks of the trade (pp. 9-48). Springer, Berlin, Heidelberg.</li>
</ul>

<p>Since variance grows with number of inputs. This makes it constant xD
It draws samples from a truncated normal distribution centered on 0 with stddev &lt;- sqrt(1 / fan_in) where fan_in is the number of input units in the weight tensor.</p>

<div class="language-julia highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">using</span> <span class="n">Distributions</span>
<span class="n">lecun_normal</span><span class="x">(</span><span class="n">fan_in</span><span class="x">)</span> <span class="o">=</span> <span class="k">return</span> <span class="n">Distributions</span><span class="o">.</span><span class="n">Normal</span><span class="x">(</span><span class="mi">0</span><span class="x">,</span> <span class="n">sqrt</span><span class="x">(</span><span class="mi">1</span><span class="o">/</span><span class="n">fan_in</span><span class="x">))</span>
<span class="n">W</span> <span class="o">=</span> <span class="n">rand</span><span class="x">(</span><span class="n">lecun_normal</span><span class="x">(</span><span class="mi">2</span><span class="x">),</span> <span class="mi">2</span><span class="x">,</span> <span class="mi">100</span><span class="x">)</span>
<span class="n">b</span> <span class="o">=</span> <span class="n">rand</span><span class="x">(</span><span class="n">lecun_normal</span><span class="x">(</span><span class="mi">2</span><span class="x">),</span> <span class="mi">2</span><span class="x">)</span>
</code></pre></div></div>

<ul>
  <li>Xavier Intialization (glorot init)
X. Glorot and Y. Bengio, “Understanding the difficulty of training deep feedforward neural networks,” in International conference on artificial intelligence and statistics, 2010, pp. 249–256.</li>
</ul>

<p>This works better with Sigmoid activations.</p>

<p>There are two of them. Xavier normal and Xavier uniform.
First Xavier Normal - It draws samples from a truncated normal distribution centered on 0 with stddev = sqrt(2 / (fan_in + fan_out)) where fan_in is the number of input units in the weight tensor and fan_out is the number of output units in the weight tensor.</p>

<div class="language-julia highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">xavier_normal</span><span class="x">(</span><span class="n">fan_in</span><span class="x">,</span><span class="n">fan_out</span><span class="x">)</span> <span class="o">=</span> <span class="k">return</span> <span class="n">Distributions</span><span class="o">.</span><span class="n">Normal</span><span class="x">(</span><span class="mi">0</span><span class="x">,</span> <span class="n">sqrt</span><span class="x">(</span><span class="mi">2</span><span class="o">/</span><span class="x">(</span><span class="n">fan_in</span><span class="o">+</span><span class="n">fan_out</span><span class="x">)))</span>

<span class="n">W</span> <span class="o">=</span> <span class="n">rand</span><span class="x">(</span><span class="n">xavier_normal</span><span class="x">(</span><span class="mi">2</span><span class="x">,</span><span class="mi">100</span><span class="x">),</span> <span class="mi">2</span><span class="x">,</span> <span class="mi">100</span><span class="x">)</span>
<span class="n">b</span> <span class="o">=</span> <span class="n">rand</span><span class="x">(</span><span class="n">xavier_normal</span><span class="x">(</span><span class="mi">2</span><span class="x">,</span><span class="mi">2</span><span class="x">),</span> <span class="mi">2</span><span class="x">)</span>
</code></pre></div></div>

<p>Now Xavier Uniform - It draws samples from a uniform distribution within -limit, limit where limit is sqrt(6 / (fan_in + fan_out)) where fan_in is the number of input units in the weight tensor and fan_out is the number of output units in the weight tensor.</p>

<div class="language-julia highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">function</span><span class="nf"> xavier_uniform</span><span class="x">(</span><span class="n">fan_in</span><span class="x">,</span><span class="n">fan_out</span><span class="x">)</span>
    <span class="n">limit</span> <span class="o">=</span> <span class="n">sqrt</span><span class="x">(</span><span class="mi">6</span><span class="o">/</span><span class="x">(</span><span class="n">fan_in</span><span class="o">+</span><span class="n">fan_out</span><span class="x">))</span>
    <span class="k">return</span> <span class="n">Distributions</span><span class="o">.</span><span class="n">Uniform</span><span class="x">(</span><span class="o">-</span><span class="n">limit</span><span class="x">,</span> <span class="n">limit</span><span class="x">)</span>
<span class="k">end</span>

<span class="n">W</span> <span class="o">=</span> <span class="n">rand</span><span class="x">(</span><span class="n">xavier_uniform</span><span class="x">(</span><span class="mi">2</span><span class="x">,</span><span class="mi">100</span><span class="x">),</span> <span class="mi">2</span><span class="x">,</span> <span class="mi">100</span><span class="x">)</span>
<span class="n">b</span> <span class="o">=</span> <span class="n">rand</span><span class="x">(</span><span class="n">xavier_uniform</span><span class="x">(</span><span class="mi">2</span><span class="x">,</span><span class="mi">2</span><span class="x">),</span> <span class="mi">2</span><span class="x">)</span>
</code></pre></div></div>

<ul>
  <li>Kaiming Initialization (he init)
K. He, X. Zhang, S. Ren, and J. Sun, “Delving Deep into Rectifiers: Surpassing Human-Level Performance on ImageNet Classification,” arXiv:1502.01852 [cs], Feb. 2015.</li>
</ul>

<p>This works better with ReLU/Leaky ReLU activations. This is mostly used everywhere because we use ReLU more than Sigmoid now.
Wow. This is just different from Xavier in the fact that there is no fan out :/ And here I thought it was some complicated thing.</p>

<p>He Normal - It draws samples from a truncated normal distribution centered on 0 with stddev = sqrt(2 / fan_in) where fan_in is the number of input units in the weight tensor.</p>

<div class="language-julia highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">he_normal</span><span class="x">(</span><span class="n">fan_in</span><span class="x">)</span> <span class="o">=</span> <span class="k">return</span> <span class="n">Distributions</span><span class="o">.</span><span class="n">Normal</span><span class="x">(</span><span class="mi">0</span><span class="x">,</span> <span class="n">sqrt</span><span class="x">(</span><span class="mi">2</span><span class="o">/</span><span class="x">(</span><span class="n">fan_in</span><span class="x">)))</span>

<span class="n">W</span> <span class="o">=</span> <span class="n">rand</span><span class="x">(</span><span class="n">he_normal</span><span class="x">(</span><span class="mi">2</span><span class="x">),</span> <span class="mi">2</span><span class="x">,</span> <span class="mi">100</span><span class="x">)</span>
<span class="n">b</span> <span class="o">=</span> <span class="n">rand</span><span class="x">(</span><span class="n">he_normal</span><span class="x">(</span><span class="mi">2</span><span class="x">),</span> <span class="mi">2</span><span class="x">)</span>
</code></pre></div></div>

<p>He uniform - It draws samples from a uniform distribution within -limit, limit where limit is sqrt(6 / fan_in) where fan_in is the number of input units in the weight tensor.</p>

<div class="language-julia highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c">#export</span>
<span class="k">function</span><span class="nf"> he_uniform</span><span class="x">(</span><span class="n">fan_in</span><span class="x">)</span>
    <span class="n">limit</span> <span class="o">=</span> <span class="n">sqrt</span><span class="x">(</span><span class="mi">6</span><span class="o">/</span><span class="x">(</span><span class="n">fan_in</span><span class="x">))</span>
    <span class="k">return</span> <span class="n">Distributions</span><span class="o">.</span><span class="n">Uniform</span><span class="x">(</span><span class="o">-</span><span class="n">limit</span><span class="x">,</span> <span class="n">limit</span><span class="x">)</span>
<span class="k">end</span>

<span class="n">W</span> <span class="o">=</span> <span class="n">rand</span><span class="x">(</span><span class="n">he_uniform</span><span class="x">(</span><span class="mi">2</span><span class="x">),</span> <span class="mi">2</span><span class="x">,</span> <span class="mi">100</span><span class="x">)</span>
<span class="n">b</span> <span class="o">=</span> <span class="n">rand</span><span class="x">(</span><span class="n">he_uniform</span><span class="x">(</span><span class="mi">2</span><span class="x">),</span> <span class="mi">2</span><span class="x">)</span>
</code></pre></div></div>

<ul>
  <li>LSUV
Mishkin, D., &amp; Matas, J. (2015). All you need is a good init. arXiv preprint arXiv:1511.06422.</li>
</ul>

<p>We cannot implement this yet because it requires us to hook into the model while it is training ):
But all it does is when the mean of the current output is &gt; 1e^-3 then we subtract the mean from the bias.
If the current outputs standard deviation -1 is &gt; 1e^-3 then we divide the weight by the standard deviation.</p>




  <small>tags: <em>inititialize</em> - <em>xavier</em> - <em>he</em> - <em>glorot</em> - <em>kaiming</em> - <em>normal</em> - <em>random</em> - <em>uniform</em> - <em>zeros</em> - <em>lsuv</em></small>



      </section>
      <footer>
        
        <p>This project is maintained by <a href="https://github.com/SubhadityaMukherjee">SubhadityaMukherjee</a></p>
        
      </footer>
    </div>
    <script src="/assets/js/scale.fix.js"></script>
    
  </body>
</html>
