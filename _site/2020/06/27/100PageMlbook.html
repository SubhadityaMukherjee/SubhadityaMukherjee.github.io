<!DOCTYPE html>
<html lang="en-US">
  <head>
    <meta charset="UTF-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/katex.min.css" integrity="sha384-zB1R0rpPzHqg7Kpt0Aljp8JPLqbXI3bhnPWROx27a9N0Ll6ZP/+DiW/UqRcLbRjq" crossorigin="anonymous">

    <!-- The loading of KaTeX is deferred to speed up page rendering -->
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/katex.min.js" integrity="sha384-y23I5Q6l+B6vatafAwxRu/0oK/79VlbSz7Q9aiSZUvyWYIYsd+qj+o24G5ZU2zJz" crossorigin="anonymous"></script>

    <!-- To automatically render math in text elements, include the auto-render extension: -->
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/contrib/auto-render.min.js" integrity="sha384-kWPLUVMOks5AQFrykwIup5lo0m3iMkkHrD0uJ4H5cjeGihAutqP0yW0J6dpFiVkI" crossorigin="anonymous"
        onload="renderMathInElement(document.body);"></script>


<!-- Begin Jekyll SEO tag v2.6.1 -->
<title>Book notes -&gt; 100PageMlblook | Deconstructing Deep learning</title>
<meta name="generator" content="Jekyll v3.8.7" />
<meta property="og:title" content="Book notes -&gt; 100PageMlblook" />
<meta name="author" content="Subhaditya Mukherjee" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="I decided to add notes to this blog too. All such notes will be tagged with “book” for easier search. This one is my notes while reading “Andriy Burkov : The Hundred-Page Machine Learning Book”. Amazon. Do support the author if you can." />
<meta property="og:description" content="I decided to add notes to this blog too. All such notes will be tagged with “book” for easier search. This one is my notes while reading “Andriy Burkov : The Hundred-Page Machine Learning Book”. Amazon. Do support the author if you can." />
<link rel="canonical" href="http://localhost:4000/2020/06/27/100PageMlbook.html" />
<meta property="og:url" content="http://localhost:4000/2020/06/27/100PageMlbook.html" />
<meta property="og:site_name" content="Deconstructing Deep learning" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2020-06-27T11:44:28+04:00" />
<script type="application/ld+json">
{"@type":"BlogPosting","url":"http://localhost:4000/2020/06/27/100PageMlbook.html","headline":"Book notes -&gt; 100PageMlblook","dateModified":"2020-06-27T11:44:28+04:00","datePublished":"2020-06-27T11:44:28+04:00","mainEntityOfPage":{"@type":"WebPage","@id":"http://localhost:4000/2020/06/27/100PageMlbook.html"},"author":{"@type":"Person","name":"Subhaditya Mukherjee"},"description":"I decided to add notes to this blog too. All such notes will be tagged with “book” for easier search. This one is my notes while reading “Andriy Burkov : The Hundred-Page Machine Learning Book”. Amazon. Do support the author if you can.","@context":"https://schema.org"}</script>
<!-- End Jekyll SEO tag -->

    <link rel="stylesheet" href="/assets/css/style.css?v=d75938eb263a78ce6542fad6f5225ab78d247a88">
    <!--[if lt IE 9]>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7.3/html5shiv.min.js"></script>
    <![endif]-->
  </head>
  <body>
    <div class="wrapper">
      <header>
        <h1><a href="https://subhadityamukherjee.github.io/deconstructingdl.html">Home</a></h1>
	<h1><a href="https://subhadityamukherjee.github.io/">About me</a></h1>
	<h1><a href = "mailto: msubhaditya@gmail.com">Drop me an email</a></h1>

        <p>Making a Deep Learning library from scratch in Julia and documenting it the whole way!</p>

        
        <p class="view"><a href="https://github.com/SubhadityaMukherjee/DataLoader.jl">View the Project on GitHub <small>github.com/SubhadityaMukherjee/DataLoader.jl</small></a></p>
        

        

        
      </header>
      <section>
      <!-- Html Elements for Search -->
<div id="search-container">
Search for something in the blog <input type="text" id="search-input" placeholder="search...">
<ul id="results-container"></ul>
</div>

<!-- Script pointing to search-script.js -->
<script src="js/search-script.js" type="text/javascript"></script>

<!-- Configuration -->
<script>
SimpleJekyllSearch({
  searchInput: document.getElementById('search-input'),
  resultsContainer: document.getElementById('results-container'),
  json: '/search.json'
})
</script>

      <small>27 June 2020</small>
<h1>Book notes -> 100PageMlblook</h1>

<p class="view">by Subhaditya Mukherjee</p>

<p>I decided to add notes to this blog too. All such notes will be tagged with “book” for easier search.
This one is my notes while reading “Andriy Burkov : The Hundred-Page Machine Learning Book”. <a href="http://themlbook.com/">Amazon</a>. Do support the author if you can.</p>

<p>A quick note on how I make notes. I first annotate the pdf of the book. And then type down the text to make it searchable. Yes I probably could use OCR but this helps me remember more. Also, this is not meant to be comprehensive reviews but only what I find interesting from the book. I read a lot about Deep Learning so these will keep popping up.</p>

<p>Okay now let us get to it :)</p>

<h2 id="initial-thoughts-from-the-content">Initial thoughts from the content</h2>
<ul>
  <li>Seems like a book which summarizes ML and tiny bit of DL</li>
  <li>Not in depth but more of an executive summary of sorts</li>
  <li>Most of the major algorithms explained in brief</li>
  <li>Bits of extra information scattered here and there</li>
</ul>

<h2 id="notes">Notes</h2>
<ul>
  <li>I skipped making notes of anything I knew prior. So these points are things that I wanted to read again or just found interesting while I was reading the book.</li>
  <li>I skipped things like linear regression while making notes so if you dont know what those are better read the book :)</li>
  <li>Why ML -&gt; Solve practical problems</li>
</ul>

<h3 id="svm">SVM</h3>
<ul>
  <li>SVM sees feature vectors as high dimensional spaces and puts them on a n dimensional plot with an n dimensional hyperplace</li>
  <li>minimize euclidean norm</li>
  <li>kernels that make boundaries non linear</li>
  <li>look for largest margin</li>
  <li>Hinge loss -&gt; if data is not linearly separable. penalizes the side of the decision boundary</li>
  <li>SVMs with hinge -&gt; soft margin. normal -&gt; hard margin</li>
  <li>largin margin : generalization</li>
  <li>kernel trick -&gt; implicitly transform original space into a higher dimensional space</li>
  <li>lagrange multipliers -&gt; optimization problem by finding equivalent representation -&gt; can be solved by quadratic algos</li>
  <li>RBF most widely used</li>
</ul>

<h3 id="random-variable">Random variable</h3>
<ul>
  <li>Prob distribution -&gt; list of prov associated with each possible value -&gt; prob mass function</li>
  <li>continuous random variable -&gt; inf possible values in interval -&gt; prob density function</li>
  <li>expectation -&gt; mean of random variable</li>
</ul>

<h3 id="unbiased-estimator">Unbiased estimator</h3>
<ul>
  <li>Unlimited no of unbiased estimators -&gt; mean will give actual value.</li>
</ul>

<h3 id="shallow-learning">Shallow learning</h3>
<ul>
  <li>Learns parameters directly from features.</li>
  <li>Vs DL -&gt; learnt from outputs of previous layers</li>
</ul>

<h3 id="cost-func">Cost func</h3>
<ul>
  <li>avg loss -&gt; empirical risk</li>
</ul>

<h3 id="decision-tree">Decision tree</h3>
<ul>
  <li>acyclic graph</li>
  <li>in each branch, specific feature is examined</li>
  <li>choose next leaf based on threshold</li>
  <li>ID3 is approximated by constructing a non parametric model</li>
  <li>recursively continue</li>
  <li>Entropy is an uncertainty measure -&gt; max when all random values have equal probability</li>
</ul>

<h3 id="gd">GD</h3>
<ul>
  <li>SGD -&gt; uses batches to compute gradient</li>
  <li>adagrad -&gt; scales ¦Á for each parameter wrt history</li>
  <li>momentum -&gt; accelerate SGD</li>
</ul>

<h3 id="techniques">Techniques</h3>
<ul>
  <li>Binning -&gt; convert continous feature into multiple binary ones</li>
  <li>Normalize -&gt; Increase speed</li>
  <li>Standardization -&gt; scale between ¦Ì and ¦Ò</li>
</ul>

<h3 id="data-imputation">Data imputation</h3>
<ul>
  <li>same value outside normal range</li>
  <li>avg value</li>
  <li>use regression to fix</li>
</ul>

<h3 id="regularization">Regularization</h3>
<ul>
  <li>L1 -&gt; sparse model,lasso reg</li>
  <li>L2 -&gt; feature selection, ridge reg</li>
</ul>

<h3 id="hyper-param">Hyper param</h3>
<ul>
  <li>Grid search</li>
  <li>Bayesian optimization</li>
  <li>Evolutionary optimization</li>
</ul>

<h3 id="rnn">RNN</h3>
<ul>
  <li>Sequence</li>
  <li>not feed forward -&gt; loops</li>
  <li>each unit gets 2 inps -&gt; vector of outputs from prev layer, vector of states from prev time step</li>
  <li>backprop through time</li>
  <li>gated RNN -&gt; forget gate</li>
  <li>store info for future use</li>
  <li>read write and erase info stored in units</li>
</ul>

<h3 id="seq2seq">Seq2seq</h3>
<ul>
  <li>Encoder -&gt; generate state with meaning representation -&gt; embedding</li>
  <li>decoder -&gt; take embedding and give output</li>
  <li>best results with attention</li>
</ul>

<h3 id="ensemble">Ensemble</h3>
<ul>
  <li>Train many low accuracy models and combine</li>
</ul>

<h3 id="other-learnings">Other learnings</h3>
<ul>
  <li>Active learning -&gt; label add to those which contribute most to model. Either density (how many examples around x) or uncertainty (how uncertain prediction of model)</li>
  <li>SVM -&gt; Use svm to predict differences and get them annotated</li>
</ul>

<h3 id="semi-supervised">Semi supervised</h3>
<ul>
  <li>self learning</li>
  <li>autoencoder</li>
  <li>bottleneck layer -&gt; embedding</li>
  <li>denoising -&gt; corrupts left hand side with random peturbation/ normal gaussian noise</li>
</ul>

<h3 id="zero-shot">Zero shot</h3>
<ul>
  <li>use embeddings to represent input x and also output y</li>
</ul>

<h3 id="combine-models">Combine models</h3>
<ul>
  <li>Average</li>
  <li>majority vote</li>
  <li>Stack -&gt; Use stacked model to tune hyper params</li>
</ul>

<h3 id="other-stuff">Other stuff</h3>
<ul>
  <li>regularization -&gt; dropout, batch norm, early stop</li>
  <li>avoid loops</li>
  <li>density estimation -&gt; model probablity density fn -&gt; novelty</li>
  <li>DBSCAN -&gt; build clusters with arbitrary shape</li>
  <li>Gaussian mixture model -&gt; member of several clusters with diff membership score</li>
  <li>UMAP seems to be better then tsne :o</li>
  <li>Ranking -&gt; LambdaMart -&gt; optimize lists on metric. eg Mean average precision (MAP)</li>
</ul>



  <small>tags: <em>review notes book 100 page machine learning svm random variable unbiased estimator shallow cost decision id3 gd sdg bin norma standa impu regu hyper rnn seq ensem active semi zero combine vote stack dbsc umap</em></small>



      </section>
      <footer>
       
        <p>This project is maintained by <a href="https://github.com/SubhadityaMukherjee">SubhadityaMukherjee</a></p>
   
      </footer>
    </div>
    <script src="/assets/js/scale.fix.js"></script>
    
  </body>
</html>
