<!DOCTYPE html>
<html lang="en-US">
  <head>
    <meta charset="UTF-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1">

<script type="text/javascript" async
  src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.4/MathJax.js?config=TeX-MML-AM_CHTML">

</script>



    <!-- To automatically render math in text elements, include the auto-render extension: --> 

<!-- Begin Jekyll SEO tag v2.6.1 -->
<title>Deconstructing Deep learning | Making a Deep Learning library from scratch in Julia and documenting it the whole way!</title>
<meta name="generator" content="Jekyll v3.9.0" />
<meta property="og:title" content="Deconstructing Deep learning" />
<meta name="author" content="Subhaditya Mukherjee" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="Making a Deep Learning library from scratch in Julia and documenting it the whole way!" />
<meta property="og:description" content="Making a Deep Learning library from scratch in Julia and documenting it the whole way!" />
<link rel="canonical" href="http://localhost:4000/deconstructingdl.html" />
<meta property="og:url" content="http://localhost:4000/deconstructingdl.html" />
<meta property="og:site_name" content="Deconstructing Deep learning" />
<script type="application/ld+json">
{"author":{"@type":"Person","name":"Subhaditya Mukherjee"},"url":"http://localhost:4000/deconstructingdl.html","description":"Making a Deep Learning library from scratch in Julia and documenting it the whole way!","headline":"Deconstructing Deep learning","@type":"WebPage","@context":"https://schema.org"}</script>
<!-- End Jekyll SEO tag -->

    <link rel="stylesheet" href="/assets/css/style.css?v=">
    <!--[if lt IE 9]>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7.3/html5shiv.min.js"></script>
    <![endif]-->
  </head>
  <body>
    <div class="wrapper">
      <header>
        <h2><a id = "imp" href="http://localhost:4000/">Home page</a></h2>
        
        
        
        
        <p>Deconstructing Deep Learning +  δeviations</p>
        
        <p>
          Drop me an <a href = "mailto: msubhaditya@gmail.com">email</a>
          | RSS feed link : <a href ="https://subhadityamukherjee.github.io/feed.xml">Click</a> | Source? Refer to<a href="https://github.com/SubhadityaMukherjee/DataLoader.jl"> the repository</a><br>
          Format : 
          Date | Title<br>
          &emsp;&emsp;TL; DR<br>
          &#9998;: Post, &#9883; : Deviation, &#128214;: Book notes, &#128640;: Spaceship
          <h4>Total number of posts : 85</h4>
         Go To : <a style="font-size:20px;color:white;" href="#PAPER">PAPER</a> o
        <a style="font-size:20px;color:white;" href="#ARTICLE">ARTICLE</a> o
        <a style="font-size:20px;color:white;" href="#SPACE">SPACE</a>
         
          <div id="search-container">
            Search for something in the blog <input type="text" id="search-input" placeholder="search...">
          </div><br>
          <ul id="results-container"></ul>
        </p>
        
        
        <p class="view"><a href="https://www.github.com/SubhadityaMukherjee">View My GitHub Profile</a></p>
      </header>
      
      <hr>
      <!-- Html Elements for Search -->
      
      <!-- Script pointing to search-script.js -->
      <script src="js/search-script.js" type="text/javascript"></script>
      
      <!-- Configuration -->
      <script>
        SimpleJekyllSearch({
          searchInput: document.getElementById('search-input'),
          resultsContainer: document.getElementById('results-container'),
          json: '/search.json'
        })
        </script>
<section>
  
<h2 id="ARTICLE"> ARTICLE </h2>
<ul>

  
  <li>
  
	    <h3><a href="/article/2020/10/09/SuperRes.html">&#9883; 9 Oct 20 : Super resolution</a></h3>
	<p>Today we will look at Super Resolution in Python.</p>


	
	
  </li>
  

  
  <li>
  
	    <h3><a href="/article/2020/10/07/FederatedLearning.html">&#9883; 7 Oct 20 : Federated learning</a></h3>
	<p>Today we will talk about Federated Learning</p>


	
	
  </li>
  

  
  <li>
  
	    <h3><a href="/article/2020/10/03/TakingBatchnormForGranted.html">&#9883; 3 Oct 20 : Taking batchnorm for granted</a></h3>
	<p>Here we will see what happens when we “dont” take Batchnorm for granted.</p>


	
	
  </li>
  

  
  <li>
  
	    <h3><a href="/article/2020/09/28/AdversarialAttack.html">&#9883; 28 Sep 20 : A murder mystery and adversarial attack</a></h3>
	<p>How smart are neural networks? And can we break them and fool them into doing dumb things?</p>


	
	
  </li>
  

  
  <li>
  
	    <h3><a href="/article/2020/09/26/An.html">&#9883; 26 Sep 20 : Thank you and a rain check</a></h3>
	<p>Hello dear reader, I share a quick rain check and announce my medium page :)</p>


	
	
  </li>
  

  
  <li>
  
	    <h3><a href="/article/2020/09/25/Pruning.html">&#9883; 25 Sep 20 : Pruning</a></h3>
	<p>Today we will look at pruning and the different approaches followed.</p>


	
	
  </li>
  

  
  <li>
  
	    <h3><a href="/article/2020/09/05/Documentation.html">&#9883; 5 Sep 20 : Documentation using documenter.jl</a></h3>
	<p>Here we will talk about how to document your code using Documenter.jl and a few tips along the way.</p>


	
	
  </li>
  

  
  <li>
  
	    <h3><a href="/article/2020/09/02/DatasetBindings.html">&#9883; 2 Sep 20 : Dataset bindings</a></h3>
	<p>Tiny post on datasets and a unified downloader for standard ones.</p>


	
	
  </li>
  

  
  <li>
  
	    <h3><a href="/article/2020/08/26/DCGAN.html">&#9883; 26 Aug 20 : Dcgan</a></h3>
	<p>Here we will talk about Generative Networks and implement a simple version of DC GAN.</p>


	
	
  </li>
  

  
  <li>
  
	    <h3><a href="/article/2020/08/13/Differentiable-Programming.html">&#9883; 13 Aug 20 : Differentiable programming</a></h3>
	<p>Deep Learning is dead. Hello Differentiable Programming. (Uh come on man)</p>


	
	
  </li>
  

  
  <li>
  
	    <h3><a href="/article/2020/08/12/Compositional-Pattern-Producing-Networks.html">&#9883; 12 Aug 20 : Compositional pattern producing networks</a></h3>
	<p>Today we will talk about Compositional Pattern Producing Networks.</p>


	
	
  </li>
  

  
  <li>
  
	    <h3><a href="/article/2020/08/11/VGG.html">&#9883; 11 Aug 20 : Vgg</a></h3>
	<p>Here we will talk about VGG networks and how to implement VGG16 and VGG19.</p>


	
	
  </li>
  

  
  <li>
  
	    <h3><a href="/article/2020/07/29/Optimizers.html">&#9883; 29 Jul 20 : Optimizers</a></h3>
	<p>Finally let us look at optimizers. Once that is done, we will be able to use Flux ML for a lot of things directly.</p>


	
	
  </li>
  

  
  <li>
  
	    <h3><a href="/article/2020/07/25/VAE.html">&#9883; 25 Jul 20 : Vae</a></h3>
	<p>A simple Variational Auto Encoder using just what we made so far!!</p>


	
	
  </li>
  

  
  <li>
  
	    <h3><a href="/article/2020/07/25/convFlux.html">&#9883; 25 Jul 20 : Simple conv with flux</a></h3>
	<p>Using the library functions which we defined till now to run a simple Neural Network.</p>


	
	
  </li>
  

  
  <li>
  
	    <h3><a href="/article/2020/07/24/rnnFromScratch.html">&#9883; 24 Jul 20 : Rnn from scratch</a></h3>
	<p>(WIP Skip for now) NLP time!! Here we will look at an RNN from scratch.</p>


	
	
  </li>
  

  
  <li>
  
	    <h3><a href="/article/2020/07/24/backprop.html">&#9883; 24 Jul 20 : Backprop</a></h3>
	<p>Looking at backpropagation from scratch because somehow I have not done that yet.</p>


	
	
  </li>
  

  
  <li>
  
	    <h3><a href="/article/2020/07/21/Low-Poly.html">&#9883; 21 Jul 20 : Low poly</a></h3>
	<p>Convert a video to low poly :) (See images below if you dont know what that is)</p>


	
	
  </li>
  

  
  <li>
  
	    <h3><a href="/article/2020/07/18/Universal-Approximation-theorem.html">&#9883; 18 Jul 20 : Universal approximation theorem</a></h3>
	<p>What makes Neural Networks tick mathematically.</p>


	
	
  </li>
  

  
  <li>
  
	    <h3><a href="/article/2020/07/17/preprocessing.html">&#9883; 17 Jul 20 : Image preprocessing</a></h3>
	<p>We look at some image processing techniques and try to implement them from scratch.</p>


	
	
  </li>
  

  
  <li>
  
	    <h3><a href="/article/2020/07/12/fastai-Book.html">&#9883; 12 Jul 20 : Fastai book</a></h3>
	<p>What I found interesting from the book “Deep Learning for Coders with fastai and PyTorch: AI Applications Without a PhD”
by Jeremy Howard  (Author), Sylvain Gugger (Author)</p>


	
	
  </li>
  

  
  <li>
  
	    <h3><a href="/article/2020/07/11/pooling.html">&#9883; 11 Jul 20 : Pooling layers</a></h3>
	<p>Here we will look at the pooling operation and its types.</p>


	
	
  </li>
  

  
  <li>
  
	    <h3><a href="/article/2020/07/10/evenFasterConv.html">&#9883; 10 Jul 20 : My life is a lie + even faster conv</a></h3>
	<p>Why CNNs are Correlation Neural Networks and an even faster Convolution operation.</p>


	
	
  </li>
  

  
  <li>
  
	    <h3><a href="/article/2020/07/09/mandelbrot.html">&#9883; 9 Jul 20 : Mandelbrot set</a></h3>
	<p>Have you heard of fractal patterns? Here we will try to make some :) (look at the pictures at the end)</p>


	
	
  </li>
  

  
  <li>
  
	    <h3><a href="/article/2020/07/07/Action-Recognition-part-2.html">&#9883; 7 Jul 20 : Action recognition part 2</a></h3>
	<p>Continuing the action recognition project.</p>


	
	
  </li>
  

  
  <li>
  
	    <h3><a href="/article/2020/07/06/Video-Classification.html">&#9883; 6 Jul 20 : Action recognition part 1</a></h3>
	<p>I try to reimplement Video recognition from <a href="https://github.com/jfzhang95/pytorch-video-recognition">Link</a> and explain the code as I go along.</p>


	
	
  </li>
  

  
  <li>
  
	    <h3><a href="/article/2020/07/06/Endangered-Species.html">&#9883; 6 Jul 20 : Endangered species</a></h3>
	<p>Here we will talk about spreading awareness about endangered species through AI powered art.</p>


	
	
  </li>
  

  
  <li>
  
	    <h3><a href="/article/2020/07/04/padding.html">&#9883; 4 Jul 20 : Padding</a></h3>
	<p>To implement a faster conv we need padding, so here we will try to explore what that means and try to implement it.</p>


	
	
  </li>
  

  
  <li>
  
	    <h3><a href="/article/2020/07/03/fasterConv.html">&#9883; 3 Jul 20 : Faster and more general conv</a></h3>
	<p>Here we will look at the various ways of implementing convolutions and benchmark them.</p>


	
	
  </li>
  

  
  <li>
  
	    <h3><a href="/article/2020/07/02/kernels.html">&#9883; 2 Jul 20 : Image kernels</a></h3>
	<p>Image kernels are fun as filters, so let us just look at a few of them and maybe try something else?</p>


	
	
  </li>
  

  
  <li>
  
	    <h3><a href="/article/2020/07/02/convolution.html">&#9883; 2 Jul 20 : Basic convolution</a></h3>
	<p>Here we will talk about convolution and how to implement it from scratch.</p>


	
	
  </li>
  

  
  <li>
  
	    <h3><a href="/article/2020/07/01/time.html">&#9883; 1 Jul 20 : Imposter syndrome in the research community</a></h3>
	<p>How to deal with imposter syndrome and what causes it.</p>


	
	
  </li>
  

  
  <li>
  
	    <h3><a href="/article/2020/06/30/latex-from-code.html">&#9883; 30 Jun 20 : Latex from code</a></h3>
	<p>I want to talk about how to get these beautiful looking latex equations without any effort at all.</p>


	
	
  </li>
  

  
  <li>
  
	    <h3><a href="/article/2020/06/30/loss-functions.html">&#9883; 30 Jun 20 : Loss functions</a></h3>
	<p>In this post we shall explore as many loss functions as I can find.</p>


	
	
  </li>
  

  
  <li>
  
	    <h3><a href="/article/2020/06/27/Implementing-Papers.html">&#9883; 27 Jun 20 : Implementing papers</a></h3>
	<p>Notes for papers I read?</p>


	
	
  </li>
  

  
  <li>
  
	    <h3><a href="/article/2020/06/27/100PageMlbook.html">&#9883; 27 Jun 20 : 100pagemlblook</a></h3>
	<p>Notes from 100 Page ML Book</p>


	
	
  </li>
  

  
  <li>
  
	    <h3><a href="/article/2020/06/24/SGD.html">&#9883; 24 Jun 20 : Sgd</a></h3>
	<p>In this post we will try to implement SGD and read a bit about what it is.</p>


	
	
  </li>
  

  
  <li>
  
	    <h3><a href="/article/2020/06/23/batching.html">&#9883; 23 Jun 20 : Batching</a></h3>
	<p>Implementing batching for large data.</p>


	
	
  </li>
  

  
  <li>
  
	    <h3><a href="/article/2020/06/21/initialization.html">&#9883; 21 Jun 20 : Initialization</a></h3>
	<p>We explore the different initialization techniques that we have and look at papers to see which does better.</p>


	
	
  </li>
  

  
  <li>
  
	    <h3><a href="/article/2020/06/20/layers.html">&#9883; 20 Jun 20 : Linear model</a></h3>
	<p>Let us start the fun with a simple model which will be extended to fit complex needs.</p>


	
	
  </li>
  

  
  <li>
  
	    <h3><a href="/article/2020/06/20/activationFunctions.html">&#9883; 20 Jun 20 : Activation functions</a></h3>
	<p>Implementing activation functions.</p>


	
	
  </li>
  

  
  <li>
  
	    <h3><a href="/article/2020/06/19/Vision.html">&#9883; 19 Jun 20 : Vision</a></h3>
	<p>A roadmap of modules I want to implement. Mostly as a todo list and a help if anyone decides they want to pitch in. (LOL)</p>


	
	
  </li>
  

  
  <li>
  
	    <h3><a href="/article/2020/06/19/notebook2script.html">&#9883; 19 Jun 20 : Notebook2script</a></h3>
	<p>I want to convert my work as a script. Selectively.</p>


	
	
  </li>
  

  
  <li>
  
	    <h3><a href="/article/2020/06/19/trainTest.html">&#9883; 19 Jun 20 : Oversample and split</a></h3>
	<p>Defining a function to split the dataset into train/test bits and oversample it as well.</p>


	
	
  </li>
  

  
  <li>
  
	    <h3><a href="/article/2020/06/19/dataloader.html">&#9883; 19 Jun 20 : Dataloader</a></h3>
	<p>The first thing we need is to be able to read data. To begin with, I am starting with the problem of image classification.</p>


	
	
  </li>
  

  
  <li>
  
	    <h3><a href="/article/2020/06/19/Initial-Steps.html">&#9883; 19 Jun 20 : Initial steps</a></h3>
	<p>What I started with and how I set up this blog</p>


	
	
  </li>
  

  
  <li>
  
	    <h3><a href="/article/2020/06/19/Defining-the-problem.html">&#9883; 19 Jun 20 : Defining the project and outlining what is to come in the future</a></h3>
	<p>An introduction to what I want to do and why.</p>


	
	
  </li>
  

</ul>

<h2 id="PAPER"> PAPER </h2>
<ul>

  
  <li>
  
	    <h3><a href="/paper/2020/06/19/swish.html">&#128214; 19 Jun 20 : Swish</a></h3>
	<p>Paper notes for the paper</p>


	
  </li>
  

  
  <li>
  
	    <h3><a href="/paper/2020/06/19/shufflenet.html">&#128214; 19 Jun 20 : Shufflenet</a></h3>
	<p>Paper notes for the paper</p>


	
  </li>
  

  
  <li>
  
	    <h3><a href="/paper/2020/06/19/selu.html">&#128214; 19 Jun 20 : Selu</a></h3>
	<p>Paper notes for the paper</p>


	
  </li>
  

  
  <li>
  
	    <h3><a href="/paper/2020/06/19/randomErasingAugmentation.html">&#128214; 19 Jun 20 : Random erasing data augmentation</a></h3>
	<p>Paper notes for the paper</p>


	
  </li>
  

  
  <li>
  
	    <h3><a href="/paper/2020/06/19/bagOfTricks.html">&#128214; 19 Jun 20 : Bag of tricks</a></h3>
	<p>Paper notes for the paper</p>


	
  </li>
  

  
  <li>
  
	    <h3><a href="/paper/2020/06/19/WGAN.html">&#128214; 19 Jun 20 : Wgan</a></h3>
	<p>Paper notes for the paper</p>


	
  </li>
  

  
  <li>
  
	    <h3><a href="/paper/2020/06/19/VGGNet.html">&#128214; 19 Jun 20 : Vgg net</a></h3>
	<p>Paper notes for the paper</p>


	
  </li>
  

  
  <li>
  
	    <h3><a href="/paper/2020/06/19/VAE.html">&#128214; 19 Jun 20 : Vae (auto-encoding variational bayes)</a></h3>
	<p>Paper notes for the paper</p>


	
  </li>
  

  
  <li>
  
	    <h3><a href="/paper/2020/06/19/Unet.html">&#128214; 19 Jun 20 : Unets</a></h3>
	<p>Paper notes for the paper</p>


	
  </li>
  

  
  <li>
  
	    <h3><a href="/paper/2020/06/19/Thinking-machines-Turing.html">&#128214; 19 Jun 20 : Thinking machines - turing (just notes)</a></h3>
	<p>Paper notes for the paper</p>


	
  </li>
  

  
  <li>
  
	    <h3><a href="/paper/2020/06/19/SuperResolution.html">&#128214; 19 Jun 20 : Super resolution using sub pixel convolutions</a></h3>
	<p>Paper notes for the paper</p>


	
  </li>
  

  
  <li>
  
	    <h3><a href="/paper/2020/06/19/SqueezeNet.html">&#128214; 19 Jun 20 : Squeeze net</a></h3>
	<p>Paper notes for the paper</p>


	
  </li>
  

  
  <li>
  
	    <h3><a href="/paper/2020/06/19/Spatial-Transformer-Network.html">&#128214; 19 Jun 20 : Spatial transformer networks</a></h3>
	<p>Paper notes for the paper</p>


	
  </li>
  

  
  <li>
  
	    <h3><a href="/paper/2020/06/19/SRCNN.html">&#128214; 19 Jun 20 : Srcnn</a></h3>
	<p>Paper notes for the paper</p>


	
  </li>
  

  
  <li>
  
	    <h3><a href="/paper/2020/06/19/Rethinking-Generalization.html">&#128214; 19 Jun 20 : Understanding deep learning requires rethinking generalization (just notes)</a></h3>
	<p>Paper notes for the paper</p>


	
  </li>
  

  
  <li>
  
	    <h3><a href="/paper/2020/06/19/ResNet.html">&#128214; 19 Jun 20 : Google net</a></h3>
	<p>Paper notes for the paper</p>


	
  </li>
  

  
  <li>
  
	    <h3><a href="/paper/2020/06/19/Pruning.html">&#128214; 19 Jun 20 : What is the state of pruning</a></h3>
	<p>Paper notes for the paper</p>


	
  </li>
  

  
  <li>
  
	    <h3><a href="/paper/2020/06/19/Perceptual-Loss.html">&#128214; 19 Jun 20 : Perceptual loss</a></h3>
	<p>Paper notes for the paper</p>


	
  </li>
  

  
  <li>
  
	    <h3><a href="/paper/2020/06/19/OneCycle.html">&#128214; 19 Jun 20 : One cycle paper</a></h3>
	<p>Paper notes for the paper</p>


	
  </li>
  

  
  <li>
  
	    <h3><a href="/paper/2020/06/19/NVAE.html">&#128214; 19 Jun 20 : Nvae (wip)</a></h3>
	<p>Paper notes for the paper</p>


	
  </li>
  

  
  <li>
  
	    <h3><a href="/paper/2020/06/19/NN-hyper-parameters-disciplined-approach.html">&#128214; 19 Jun 20 : A disciplined approach to neural network hyper-parameters</a></h3>
	<p>Paper notes for the paper</p>


	
  </li>
  

  
  <li>
  
	    <h3><a href="/paper/2020/06/19/MobileNet.html">&#128214; 19 Jun 20 : Mobile net</a></h3>
	<p>Paper notes for the paper</p>


	
  </li>
  

  
  <li>
  
	    <h3><a href="/paper/2020/06/19/LightSeg.html">&#128214; 19 Jun 20 : Lightseg (only notes for now)</a></h3>
	<p>Paper notes for the paper</p>


	
  </li>
  

  
  <li>
  
	    <h3><a href="/paper/2020/06/19/LSTM.html">&#128214; 19 Jun 20 : Lstm</a></h3>
	<p>Paper notes for the paper</p>


	
  </li>
  

  
  <li>
  
	    <h3><a href="/paper/2020/06/19/HRNet.html">&#128214; 19 Jun 20 : Hrnet (wip)</a></h3>
	<p>Paper notes for the paper</p>


	
  </li>
  

  
  <li>
  
	    <h3><a href="/paper/2020/06/19/GoogLeNet.html">&#128214; 19 Jun 20 : Google net</a></h3>
	<p>Paper notes for the paper</p>


	
  </li>
  

  
  <li>
  
	    <h3><a href="/paper/2020/06/19/Focal-Loss.html">&#128214; 19 Jun 20 : Focal loss</a></h3>
	<p>Paper notes for the paper</p>


	
  </li>
  

  
  <li>
  
	    <h3><a href="/paper/2020/06/19/FederatedLearningGboard.html">&#128214; 19 Jun 20 : Google keyboard federated learning(notes only. refer to [35] for code)</a></h3>
	<p>Paper notes for the paper</p>


	
  </li>
  

  
  <li>
  
	    <h3><a href="/paper/2020/06/19/FederatedLearning.html">&#128214; 19 Jun 20 : Federated learning (original paper)</a></h3>
	<p>Paper notes for the paper</p>


	
  </li>
  

  
  <li>
  
	    <h3><a href="/paper/2020/06/19/Dropout.html">&#128214; 19 Jun 20 : Dropout (just notes)</a></h3>
	<p>Paper notes for the paper</p>


	
  </li>
  

  
  <li>
  
	    <h3><a href="/paper/2020/06/19/DeepLab-(Semantic-seg).html">&#128214; 19 Jun 20 : Semantic segmentation deeplab</a></h3>
	<p>Paper notes for the paper</p>


	
  </li>
  

  
  <li>
  
	    <h3><a href="/paper/2020/06/19/DeepDream.html">&#128214; 19 Jun 20 : Inceptionism (google deep dream)</a></h3>
	<p>Paper notes for the paper</p>


	
  </li>
  

  
  <li>
  
	    <h3><a href="/paper/2020/06/19/DRlNeuro.html">&#128214; 19 Jun 20 : Drl and neuroscience (just notes</a></h3>
	<p>Paper notes for the paper</p>


	
  </li>
  

  
  <li>
  
	    <h3><a href="/paper/2020/06/19/DCGan.html">&#128214; 19 Jun 20 : Dc gan</a></h3>
	<p>Paper notes for the paper</p>


	
  </li>
  

  
  <li>
  
	    <h3><a href="/paper/2020/06/19/ComputationalLimits.html">&#128214; 19 Jun 20 : Computational limits (just notes)</a></h3>
	<p>Paper notes for the paper</p>


	
  </li>
  

  
  <li>
  
	    <h3><a href="/paper/2020/06/19/Class-Imbalance.html">&#128214; 19 Jun 20 : Class imbalance problem</a></h3>
	<p>Paper notes for the paper</p>


	
  </li>
  

  
  <li>
  
	    <h3><a href="/paper/2020/06/19/AlexNet.html">&#128214; 19 Jun 20 : Alex net</a></h3>
	<p>Paper notes for the paper</p>


	
  </li>
  

</ul>

<h2 id="SPACE"> SPACE </h2>
<ul>

  
  <li>
  
	    <h3><a href="/space/2020/07/07/adopt.html">&#128640; 7 Jul 20 : Adopt dont shop</a></h3>
	<p>I want to talk about adopting animals.</p>


	
  </li>
  

</ul>


      <footer>
        
      </footer>
    </div>
    <script src="/assets/js/scale.fix.js"></script>
  </body>
</html>
