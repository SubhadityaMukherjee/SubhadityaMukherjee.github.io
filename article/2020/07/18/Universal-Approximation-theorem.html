<!DOCTYPE html>
<html lang="en-US">
  <head>
    <meta charset="UTF-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1">

<script type="text/javascript" async
  src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.4/MathJax.js?config=TeX-MML-AM_CHTML">

</script>

<!-- Begin Jekyll SEO tag v2.6.1 -->
<title>Universal Approximation theorem | Deconstructing Deep learning</title>
<meta name="generator" content="Jekyll v3.9.0" />
<meta property="og:title" content="Universal Approximation theorem" />
<meta name="author" content="Subhaditya Mukherjee" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="What makes Neural Networks tick mathematically." />
<meta property="og:description" content="What makes Neural Networks tick mathematically." />
<meta property="og:site_name" content="Deconstructing Deep learning" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2020-07-18T06:42:41+00:00" />
<script type="application/ld+json">
{"description":"What makes Neural Networks tick mathematically.","url":"/article/2020/07/18/Universal-Approximation-theorem.html","@type":"BlogPosting","headline":"Universal Approximation theorem","dateModified":"2020-07-18T06:42:41+00:00","datePublished":"2020-07-18T06:42:41+00:00","author":{"@type":"Person","name":"Subhaditya Mukherjee"},"mainEntityOfPage":{"@type":"WebPage","@id":"/article/2020/07/18/Universal-Approximation-theorem.html"},"@context":"https://schema.org"}</script>
<!-- End Jekyll SEO tag -->


    <link rel="stylesheet" href="/assets/css/landing.css?v=">
  </head>
  <body>
    <div class="col-md-4">
      <header>
        <h2><a id = "imp" href="/">Home page</a></h2>
        <p>Deconstructing Deep Learning +  δeviations</p>
        
        <p>
          Drop me an <a href = "mailto: msubhaditya@gmail.com">email</a>
          | RSS feed link : <a href ="https://subhadityamukherjee.github.io/feed.xml">Click</a> <br>
          Format : 
          Date | Title<br>
          &emsp;&emsp;TL; DR<br>
          <h4>Total number of posts : 88</h4>
         Go To : <a style="font-size:20px;color:white;" href="#PAPER">PAPERS</a> o
        <a style="font-size:20px;color:white;" href="#ARTICLE">ARTICLES</a> o
        <a style="font-size:20px;color:white;" href="#BOOK">BOOKS</a> o
        <a style="font-size:20px;color:white;" href="#SPACE">SPACE</a>
         
        </p>
        
        <p class="view">
        <a href="https://www.github.com/SubhadityaMukherjee">View My GitHub Profile </a>
        </p>
      </header>
      
      <hr>
    </div>
<section>
      <div class="col-md-5">
  <a href = "/deeplearning.html">Go to index</a><br><br>


<h1>Universal Approximation theorem</h1>

<span class="reading-time" title="Estimated read time">
  
  
    <h3>Reading time : ~3 mins</h3>
  
</span>


<p class="view">by Subhaditya Mukherjee</p>
<ul>
  <li><a href="#what-is-it">What is it</a></li>
  <li><a href="#how-does-it-work">How does it work?</a></li>
</ul>

<p>What makes Neural Networks tick mathematically.</p>

<p>This is going to be a really short article but I decided it was important to talk about it. So the reason why NNs work is due to a theorem called the Universal Approximation theorem.</p>

<h2 id="what-is-it">What is it</h2>

<p>What this means that given an x and a y, the NN can identify a mapping between them. “Approximately”. This is required when we have non linearly separable data. (Aka you can’t split them directly into n parts just by say drawing a line between them. This could be complex structures like images or text or anything which cannot be directly modelled.</p>

<p>So we take a non linear function, for example the sigmoid. \(\frac{1}{1 + e^{ - \left( w^{T}x + b \right)}}\).
Then we have to combine multiple such neurons in a way such that we can accurately model our problem. The end result is a complex function and the existing weights are distributed across many layers. Sounds familiar? Welcome to Deep Learning (lol).</p>

<p>The Universal approximation theorem states that</p>
<blockquote>
  <p>a feed forward network with a single hidden layer containing a finite number of neurons can approximate continuous functions on compact subsets of \(\mathbb{R}\) , under mild assumptions on the activation function.</p>
</blockquote>

<p>Um. Can these guys speak normally. -.- Lets break it down a bit.</p>

<ul>
  <li>a feed forward network : take an input, apply a function, get an output, repeat</li>
  <li>a single hidden layer : yes you can use more, but theoretically…</li>
  <li>finite number of neurons: you can do it without needing an infinite computer</li>
  <li>approximate continuous functions: continuous functions are anything which dont have breaks/holes in between. This just says that it is possible to approximate the mapping which we talked about</li>
  <li>\(\mathbb{R}\) is just the set of all real numbers</li>
  <li>An activation function is something like the ReLU/Sigmoid</li>
  <li>All this boils down to the fact that a neural network can approximate any complex relation given an input and an output.</li>
</ul>

<h2 id="how-does-it-work">How does it work?</h2>

<p>Well here is an image. See if you can understand whats happening.
 <img src="/img/uat.png" alt="" /></p>

<p>Makes sense right? Every curve at an infinitely small point can be a collection of lines (approximately).
Oh and as a form of citation, <a href="https://medium.com/hackernoon/illustrative-proof-of-universal-approximation-theorem-5845c02822f6">here</a> is where I got this image from. Its a great blog you should really check it out.</p>


<section>
Related posts:&emsp;

  <a href=/book/2021/03/09/AISuperpowersKaiFuLee.html> AI Superpowers Kai Fu Lee&emsp; </a>

  <a href=/book/2021/03/07/DigitalMinimalismCalNewport.html> Digital Minimalism Cal Newport&emsp; </a>

  <a href=/article/2021/03/05/tricksFromLectures.html> More Deep Learning, Less Crying - A guide&emsp; </a>

  <a href=/article/2020/10/09/SuperRes.html> Super resolution&emsp; </a>

  <a href=/article/2020/10/07/FederatedLearning.html> Federated Learning&emsp; </a>

  <a href=/article/2020/10/03/TakingBatchnormForGranted.html> Taking Batchnorm For Granted&emsp; </a>

  <a href=/article/2020/09/28/AdversarialAttack.html> A murder mystery and Adversarial attack&emsp; </a>

  <a href=/article/2020/09/26/An.html> Thank you and a rain check&emsp; </a>

  <a href=/article/2020/09/25/Pruning.html> Pruning&emsp; </a>

  <a href=/article/2020/09/04/Documentation.html> Documentation using Documenter.jl&emsp; </a>


</section>


    </div>
  </body>
</html>
